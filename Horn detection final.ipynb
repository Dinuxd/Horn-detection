{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcc8e837-2e00-4b0b-8221-2e2dc3175b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in c:\\conda\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: sounddevice in c:\\conda\\lib\\site-packages (0.5.3)\n",
      "Requirement already satisfied: tensorflow==2.16.1 in c:\\conda\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\conda\\lib\\site-packages (from tensorflow==2.16.1) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\conda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\conda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\conda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\conda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\conda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\conda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\conda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\conda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\conda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\conda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\conda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\conda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\conda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\conda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\conda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\conda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\conda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\conda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (1.76.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\conda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\conda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (3.12.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\conda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (1.26.4)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\conda\\lib\\site-packages (from librosa) (3.1.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\conda\\lib\\site-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\conda\\lib\\site-packages (from librosa) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\conda\\lib\\site-packages (from librosa) (1.5.1)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\conda\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\conda\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\conda\\lib\\site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\conda\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\conda\\lib\\site-packages (from librosa) (1.0.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\conda\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\conda\\lib\\site-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\conda\\lib\\site-packages (from sounddevice) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\conda\\lib\\site-packages (from CFFI>=1.0->sounddevice) (2.21)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\conda\\lib\\site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\conda\\lib\\site-packages (from pooch>=1.1->librosa) (3.10.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\conda\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\conda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow==2.16.1) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\conda\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow==2.16.1) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\conda\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow==2.16.1) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\conda\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow==2.16.1) (0.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\conda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow==2.16.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\conda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow==2.16.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\conda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow==2.16.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\conda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow==2.16.1) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\conda\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow==2.16.1) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\conda\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow==2.16.1) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\conda\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow==2.16.1) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\conda\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow==2.16.1) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\conda\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow==2.16.1) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\conda\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow==2.16.1) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\conda\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow==2.16.1) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa sounddevice tensorflow==2.16.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5734a8f-2f5e-4f77-bce2-55a8ddadbfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.16.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0ce7fb2-5d80-41ae-b7dc-d2c77eb1e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_DIR = r\"C:/Users/Dinupa Devinda/Desktop/ML/Horn/Dataset\"   \n",
    "\n",
    "TRAIN_CSV = \"C:/Users/Dinupa Devinda/Desktop/ML/Horn/hornbase_train.csv\"\n",
    "TEST_CSV  = \"C:/Users/Dinupa Devinda/Desktop/ML/Horn/hornbase_test.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "301e9b2a-00b4-4211-bfee-5626f76a5b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            file class  class number\n",
      "0  C_10_A_L_.wav  horn             1\n",
      "1   C_10_A_S.wav  horn             1\n",
      "2   C_10_A_V.wav  horn             1\n",
      "3  C_10_A_V_.wav  horn             1\n",
      "4   C_10_B_L.wav  horn             1\n",
      "Train samples: 756\n",
      "Test samples: 324\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df  = pd.read_csv(TEST_CSV)\n",
    "\n",
    "print(train_df.head())\n",
    "print(\"Train samples:\", len(train_df))\n",
    "print(\"Test samples:\", len(test_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eba0578-1836-4db1-8990-bda7db29d96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Extracting log-mel spectrogramsâ€¦\n",
      "All train raw: (756, 128, 87)\n",
      "Test raw     : (324, 128, 87)\n",
      "Train raw: (642, 128, 87) Val raw: (114, 128, 87)\n",
      "Augmented train: (1284, 128, 87) labels: (1284,)\n",
      "Global mean/std: -43.899112701416016 19.589555741356445\n",
      "Train: (1284, 128, 87, 1) Val: (114, 128, 87, 1) Test: (324, 128, 87, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "TARGET_SR  = 44100      \n",
    "DURATION   = 1.0         \n",
    "N_SAMPLES  = int(TARGET_SR * DURATION)\n",
    "\n",
    "\n",
    "N_MELS     = 128        \n",
    "N_FFT      = 1024\n",
    "HOP_LENGTH = 512\n",
    "\n",
    "def wav_to_logmel_raw(path):\n",
    "    \"\"\"Load wav and return *unnormalized* log-mel spectrogram.\"\"\"\n",
    "    y, sr = librosa.load(path, sr=TARGET_SR, mono=True)\n",
    "\n",
    "    # Ensure exact length 1 s\n",
    "    if len(y) < N_SAMPLES:\n",
    "        y = np.pad(y, (0, N_SAMPLES - len(y)))\n",
    "    else:\n",
    "        y = y[:N_SAMPLES]\n",
    "\n",
    "    mel = librosa.feature.melspectrogram(\n",
    "        y=y,\n",
    "        sr=sr,\n",
    "        n_fft=N_FFT,\n",
    "        hop_length=HOP_LENGTH,\n",
    "        n_mels=N_MELS,\n",
    "        power=2.0\n",
    "    )\n",
    "    log_mel = librosa.power_to_db(mel, ref=np.max)\n",
    "    return log_mel.astype(\"float32\")\n",
    "\n",
    "def load_logmel_dataset(df, audio_dir):\n",
    "    \"\"\"Return (X_raw, y) where X_raw is (N, mels, frames) unnormalized.\"\"\"\n",
    "    feats = []\n",
    "    labels = []\n",
    "    for _, row in df.iterrows():\n",
    "        fname = row[\"file\"]\n",
    "        label = row[\"class number\"]   \n",
    "\n",
    "        full_path = os.path.join(audio_dir, fname)\n",
    "        if not os.path.exists(full_path):\n",
    "            raise FileNotFoundError(full_path)\n",
    "\n",
    "        logmel = wav_to_logmel_raw(full_path)\n",
    "        feats.append(logmel)\n",
    "        labels.append(label)\n",
    "\n",
    "    X_raw = np.stack(feats)                      \n",
    "    y = np.array(labels, dtype=\"float32\")\n",
    "    return X_raw, y\n",
    "\n",
    "print(\"ğŸ” Extracting log-mel spectrogramsâ€¦\")\n",
    "X_all_raw, y_all   = load_logmel_dataset(train_df, AUDIO_DIR)\n",
    "X_test_raw, y_test = load_logmel_dataset(test_df,  AUDIO_DIR)\n",
    "\n",
    "print(\"All train raw:\", X_all_raw.shape)\n",
    "print(\"Test raw     :\", X_test_raw.shape)\n",
    "\n",
    "\n",
    "X_train_raw, X_val_raw, y_train, y_val = train_test_split(\n",
    "    X_all_raw, y_all,\n",
    "    test_size=0.15,\n",
    "    stratify=y_all,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train raw:\", X_train_raw.shape, \"Val raw:\", X_val_raw.shape)\n",
    "\n",
    "\n",
    "def random_spec_augment(mel):\n",
    "    \"\"\"\n",
    "    mel: (n_mels, n_frames) log-mel\n",
    "    returns augmented copy\n",
    "    \"\"\"\n",
    "    v = mel.copy()\n",
    "    n_mels, n_frames = v.shape\n",
    "\n",
    "    # Frequency mask\n",
    "    max_f = max(1, n_mels // 8)   # up to 1/8 of bands\n",
    "    f = np.random.randint(0, max_f + 1)\n",
    "    if f > 0:\n",
    "        f0 = np.random.randint(0, n_mels - f + 1)\n",
    "        v[f0:f0+f, :] = 0.0\n",
    "\n",
    "    # Time mask\n",
    "    max_t = max(1, n_frames // 8)\n",
    "    t = np.random.randint(0, max_t + 1)\n",
    "    if t > 0:\n",
    "        t0 = np.random.randint(0, n_frames - t + 1)\n",
    "        v[:, t0:t0+t] = 0.0\n",
    "\n",
    "    return v\n",
    "\n",
    "\n",
    "X_train_aug_list = []\n",
    "y_train_aug_list = []\n",
    "\n",
    "for x, y in zip(X_train_raw, y_train):\n",
    "    X_train_aug_list.append(x)\n",
    "    y_train_aug_list.append(y)\n",
    "\n",
    "    x_aug = random_spec_augment(x)\n",
    "    X_train_aug_list.append(x_aug)\n",
    "    y_train_aug_list.append(y)\n",
    "\n",
    "X_train_aug = np.stack(X_train_aug_list)\n",
    "y_train_aug = np.array(y_train_aug_list, dtype=\"float32\")\n",
    "\n",
    "print(\"Augmented train:\", X_train_aug.shape, \"labels:\", y_train_aug.shape)\n",
    "\n",
    "\n",
    "mean = X_train_aug.mean()\n",
    "std  = X_train_aug.std() + 1e-9\n",
    "print(\"Global mean/std:\", float(mean), float(std))\n",
    "\n",
    "def normalize_and_expand(X, mean, std):\n",
    "    Xn = (X - mean) / std\n",
    "    return Xn[..., np.newaxis].astype(\"float32\") \n",
    "\n",
    "X_train = normalize_and_expand(X_train_aug, mean, std)\n",
    "X_val   = normalize_and_expand(X_val_raw,   mean, std)\n",
    "X_test  = normalize_and_expand(X_test_raw,  mean, std)\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c371ad2-8239-41c6-96e4-cb39ffdcdc04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                         </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape                </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">87</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">87</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 â”‚                             â”‚                 â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">87</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_1                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 â”‚                             â”‚                 â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_2                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 â”‚                             â”‚                 â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_3                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 â”‚                             â”‚                 â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             â”‚                             â”‚                 â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m87\u001b[0m, \u001b[38;5;34m32\u001b[0m)         â”‚             \u001b[38;5;34m320\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m87\u001b[0m, \u001b[38;5;34m32\u001b[0m)         â”‚             \u001b[38;5;34m128\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 â”‚                             â”‚                 â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ activation (\u001b[38;5;33mActivation\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m87\u001b[0m, \u001b[38;5;34m32\u001b[0m)         â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m43\u001b[0m, \u001b[38;5;34m32\u001b[0m)          â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)                    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m43\u001b[0m, \u001b[38;5;34m32\u001b[0m)          â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m43\u001b[0m, \u001b[38;5;34m64\u001b[0m)          â”‚          \u001b[38;5;34m18,496\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_1                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m43\u001b[0m, \u001b[38;5;34m64\u001b[0m)          â”‚             \u001b[38;5;34m256\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 â”‚                             â”‚                 â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ activation_1 (\u001b[38;5;33mActivation\u001b[0m)            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m43\u001b[0m, \u001b[38;5;34m64\u001b[0m)          â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m64\u001b[0m)          â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m64\u001b[0m)          â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m128\u001b[0m)         â”‚          \u001b[38;5;34m73,856\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_2                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m128\u001b[0m)         â”‚             \u001b[38;5;34m512\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 â”‚                             â”‚                 â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ activation_2 (\u001b[38;5;33mActivation\u001b[0m)            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m128\u001b[0m)         â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)         â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)         â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)         â”‚         \u001b[38;5;34m147,584\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_3                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)         â”‚             \u001b[38;5;34m512\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 â”‚                             â”‚                 â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ activation_3 (\u001b[38;5;33mActivation\u001b[0m)            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)         â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             â”‚                             â”‚                 â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 â”‚          \u001b[38;5;34m16,512\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   â”‚             \u001b[38;5;34m129\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">258,305</span> (1009.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m258,305\u001b[0m (1009.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">257,601</span> (1006.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m257,601\u001b[0m (1006.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">704</span> (2.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m704\u001b[0m (2.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.6085 - loss: 0.6887 - precision: 0.6036 - recall: 0.5456\n",
      "Epoch 1: val_loss improved from None to 0.84465, saving model to C:\\Users\\Dinupa Devinda\\Desktop\\ML\\Horn\\horn_cnn_best.keras\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 131ms/step - accuracy: 0.6472 - loss: 0.6527 - precision: 0.6621 - recall: 0.6012 - val_accuracy: 0.5000 - val_loss: 0.8446 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 2/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.7418 - loss: 0.5569 - precision: 0.7311 - recall: 0.7493\n",
      "Epoch 2: val_loss did not improve from 0.84465\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 115ms/step - accuracy: 0.7609 - loss: 0.5316 - precision: 0.7750 - recall: 0.7352 - val_accuracy: 0.5000 - val_loss: 1.0924 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 3/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8100 - loss: 0.4542 - precision: 0.8312 - recall: 0.7769\n",
      "Epoch 3: val_loss did not improve from 0.84465\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 115ms/step - accuracy: 0.8123 - loss: 0.4346 - precision: 0.8325 - recall: 0.7819 - val_accuracy: 0.5000 - val_loss: 1.3682 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 4/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8564 - loss: 0.3865 - precision: 0.8528 - recall: 0.8717\n",
      "Epoch 4: val_loss did not improve from 0.84465\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 115ms/step - accuracy: 0.8551 - loss: 0.3845 - precision: 0.8562 - recall: 0.8536 - val_accuracy: 0.5000 - val_loss: 1.8817 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 5/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8410 - loss: 0.3889 - precision: 0.8895 - recall: 0.7720\n",
      "Epoch 5: val_loss did not improve from 0.84465\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 113ms/step - accuracy: 0.8629 - loss: 0.3591 - precision: 0.9003 - recall: 0.8162 - val_accuracy: 0.5000 - val_loss: 2.1086 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 6/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8799 - loss: 0.3351 - precision: 0.8962 - recall: 0.8667\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.84465\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 113ms/step - accuracy: 0.8801 - loss: 0.3261 - precision: 0.9013 - recall: 0.8536 - val_accuracy: 0.5263 - val_loss: 2.0484 - val_precision: 1.0000 - val_recall: 0.0526 - learning_rate: 0.0010\n",
      "Epoch 7/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.9058 - loss: 0.2750 - precision: 0.9382 - recall: 0.8687\n",
      "Epoch 7: val_loss did not improve from 0.84465\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - accuracy: 0.8933 - loss: 0.2854 - precision: 0.9201 - recall: 0.8614 - val_accuracy: 0.5702 - val_loss: 1.5633 - val_precision: 1.0000 - val_recall: 0.1404 - learning_rate: 5.0000e-04\n",
      "Epoch 8/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9187 - loss: 0.2484 - precision: 0.9399 - recall: 0.8973\n",
      "Epoch 8: val_loss did not improve from 0.84465\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 116ms/step - accuracy: 0.9190 - loss: 0.2462 - precision: 0.9367 - recall: 0.8988 - val_accuracy: 0.5965 - val_loss: 1.5780 - val_precision: 1.0000 - val_recall: 0.1930 - learning_rate: 5.0000e-04\n",
      "Epoch 9/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.9105 - loss: 0.2393 - precision: 0.9302 - recall: 0.8816\n",
      "Epoch 9: val_loss did not improve from 0.84465\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - accuracy: 0.9128 - loss: 0.2472 - precision: 0.9387 - recall: 0.8832 - val_accuracy: 0.6228 - val_loss: 1.3852 - val_precision: 1.0000 - val_recall: 0.2456 - learning_rate: 5.0000e-04\n",
      "Epoch 10/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.9450 - loss: 0.1994 - precision: 0.9541 - recall: 0.9320\n",
      "Epoch 10: val_loss improved from 0.84465 to 0.81477, saving model to C:\\Users\\Dinupa Devinda\\Desktop\\ML\\Horn\\horn_cnn_best.keras\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 116ms/step - accuracy: 0.9268 - loss: 0.2159 - precision: 0.9419 - recall: 0.9097 - val_accuracy: 0.7105 - val_loss: 0.8148 - val_precision: 1.0000 - val_recall: 0.4211 - learning_rate: 5.0000e-04\n",
      "Epoch 11/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.9129 - loss: 0.2559 - precision: 0.9263 - recall: 0.8996\n",
      "Epoch 11: val_loss did not improve from 0.81477\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - accuracy: 0.9128 - loss: 0.2553 - precision: 0.9344 - recall: 0.8879 - val_accuracy: 0.7368 - val_loss: 0.8578 - val_precision: 1.0000 - val_recall: 0.4737 - learning_rate: 5.0000e-04\n",
      "Epoch 12/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9169 - loss: 0.2469 - precision: 0.9452 - recall: 0.8865\n",
      "Epoch 12: val_loss improved from 0.81477 to 0.50110, saving model to C:\\Users\\Dinupa Devinda\\Desktop\\ML\\Horn\\horn_cnn_best.keras\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 117ms/step - accuracy: 0.9307 - loss: 0.2180 - precision: 0.9525 - recall: 0.9065 - val_accuracy: 0.7895 - val_loss: 0.5011 - val_precision: 1.0000 - val_recall: 0.5789 - learning_rate: 5.0000e-04\n",
      "Epoch 13/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9361 - loss: 0.2032 - precision: 0.9553 - recall: 0.9153\n",
      "Epoch 13: val_loss improved from 0.50110 to 0.21867, saving model to C:\\Users\\Dinupa Devinda\\Desktop\\ML\\Horn\\horn_cnn_best.keras\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - accuracy: 0.9330 - loss: 0.2071 - precision: 0.9528 - recall: 0.9112 - val_accuracy: 0.8860 - val_loss: 0.2187 - val_precision: 1.0000 - val_recall: 0.7719 - learning_rate: 5.0000e-04\n",
      "Epoch 14/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9265 - loss: 0.2134 - precision: 0.9380 - recall: 0.9239\n",
      "Epoch 14: val_loss improved from 0.21867 to 0.15746, saving model to C:\\Users\\Dinupa Devinda\\Desktop\\ML\\Horn\\horn_cnn_best.keras\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 115ms/step - accuracy: 0.9291 - loss: 0.1967 - precision: 0.9311 - recall: 0.9268 - val_accuracy: 0.9474 - val_loss: 0.1575 - val_precision: 1.0000 - val_recall: 0.8947 - learning_rate: 5.0000e-04\n",
      "Epoch 15/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9490 - loss: 0.1907 - precision: 0.9694 - recall: 0.9262\n",
      "Epoch 15: val_loss did not improve from 0.15746\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step - accuracy: 0.9455 - loss: 0.1777 - precision: 0.9583 - recall: 0.9315 - val_accuracy: 0.8158 - val_loss: 0.4647 - val_precision: 1.0000 - val_recall: 0.6316 - learning_rate: 5.0000e-04\n",
      "Epoch 16/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9265 - loss: 0.2229 - precision: 0.9455 - recall: 0.9058\n",
      "Epoch 16: val_loss improved from 0.15746 to 0.15216, saving model to C:\\Users\\Dinupa Devinda\\Desktop\\ML\\Horn\\horn_cnn_best.keras\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 115ms/step - accuracy: 0.9213 - loss: 0.2222 - precision: 0.9427 - recall: 0.8972 - val_accuracy: 0.9386 - val_loss: 0.1522 - val_precision: 1.0000 - val_recall: 0.8772 - learning_rate: 5.0000e-04\n",
      "Epoch 17/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.9358 - loss: 0.1705 - precision: 0.9450 - recall: 0.9293\n",
      "Epoch 17: val_loss did not improve from 0.15216\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 115ms/step - accuracy: 0.9354 - loss: 0.1857 - precision: 0.9458 - recall: 0.9237 - val_accuracy: 0.9035 - val_loss: 0.2433 - val_precision: 1.0000 - val_recall: 0.8070 - learning_rate: 5.0000e-04\n",
      "Epoch 18/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9296 - loss: 0.2008 - precision: 0.9545 - recall: 0.9044\n",
      "Epoch 18: val_loss did not improve from 0.15216\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 113ms/step - accuracy: 0.9237 - loss: 0.2135 - precision: 0.9401 - recall: 0.9050 - val_accuracy: 0.9035 - val_loss: 0.2063 - val_precision: 1.0000 - val_recall: 0.8070 - learning_rate: 5.0000e-04\n",
      "Epoch 19/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.9378 - loss: 0.2011 - precision: 0.9796 - recall: 0.8942\n",
      "Epoch 19: val_loss improved from 0.15216 to 0.14532, saving model to C:\\Users\\Dinupa Devinda\\Desktop\\ML\\Horn\\horn_cnn_best.keras\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 115ms/step - accuracy: 0.9315 - loss: 0.2164 - precision: 0.9586 - recall: 0.9019 - val_accuracy: 0.9561 - val_loss: 0.1453 - val_precision: 1.0000 - val_recall: 0.9123 - learning_rate: 5.0000e-04\n",
      "Epoch 20/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.9376 - loss: 0.1974 - precision: 0.9640 - recall: 0.9045\n",
      "Epoch 20: val_loss did not improve from 0.14532\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 115ms/step - accuracy: 0.9385 - loss: 0.1960 - precision: 0.9653 - recall: 0.9097 - val_accuracy: 0.7632 - val_loss: 0.6167 - val_precision: 1.0000 - val_recall: 0.5263 - learning_rate: 5.0000e-04\n",
      "Epoch 21/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.9488 - loss: 0.1808 - precision: 0.9505 - recall: 0.9447\n",
      "Epoch 21: val_loss improved from 0.14532 to 0.08446, saving model to C:\\Users\\Dinupa Devinda\\Desktop\\ML\\Horn\\horn_cnn_best.keras\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 116ms/step - accuracy: 0.9470 - loss: 0.1797 - precision: 0.9644 - recall: 0.9283 - val_accuracy: 0.9825 - val_loss: 0.0845 - val_precision: 1.0000 - val_recall: 0.9649 - learning_rate: 5.0000e-04\n",
      "Epoch 22/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9567 - loss: 0.1473 - precision: 0.9669 - recall: 0.9459\n",
      "Epoch 22: val_loss did not improve from 0.08446\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 113ms/step - accuracy: 0.9548 - loss: 0.1565 - precision: 0.9665 - recall: 0.9424 - val_accuracy: 0.9386 - val_loss: 0.2463 - val_precision: 0.8906 - val_recall: 1.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 23/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.9578 - loss: 0.1398 - precision: 0.9721 - recall: 0.9434\n",
      "Epoch 23: val_loss did not improve from 0.08446\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 113ms/step - accuracy: 0.9587 - loss: 0.1433 - precision: 0.9682 - recall: 0.9486 - val_accuracy: 0.9035 - val_loss: 0.1971 - val_precision: 1.0000 - val_recall: 0.8070 - learning_rate: 5.0000e-04\n",
      "Epoch 24/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9516 - loss: 0.1424 - precision: 0.9524 - recall: 0.9512\n",
      "Epoch 24: val_loss did not improve from 0.08446\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - accuracy: 0.9657 - loss: 0.1293 - precision: 0.9731 - recall: 0.9579 - val_accuracy: 0.8947 - val_loss: 0.2521 - val_precision: 1.0000 - val_recall: 0.7895 - learning_rate: 5.0000e-04\n",
      "Epoch 25/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9660 - loss: 0.1195 - precision: 0.9632 - recall: 0.9692\n",
      "Epoch 25: val_loss did not improve from 0.08446\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 113ms/step - accuracy: 0.9673 - loss: 0.1196 - precision: 0.9702 - recall: 0.9642 - val_accuracy: 0.8860 - val_loss: 0.2594 - val_precision: 1.0000 - val_recall: 0.7719 - learning_rate: 5.0000e-04\n",
      "Epoch 26/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9597 - loss: 0.1425 - precision: 0.9441 - recall: 0.9757\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.08446\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 113ms/step - accuracy: 0.9634 - loss: 0.1302 - precision: 0.9656 - recall: 0.9611 - val_accuracy: 0.9298 - val_loss: 0.1462 - val_precision: 0.8769 - val_recall: 1.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 27/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9616 - loss: 0.1463 - precision: 0.9686 - recall: 0.9554\n",
      "Epoch 27: val_loss improved from 0.08446 to 0.07443, saving model to C:\\Users\\Dinupa Devinda\\Desktop\\ML\\Horn\\horn_cnn_best.keras\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 113ms/step - accuracy: 0.9688 - loss: 0.1289 - precision: 0.9748 - recall: 0.9626 - val_accuracy: 0.9825 - val_loss: 0.0744 - val_precision: 0.9661 - val_recall: 1.0000 - learning_rate: 2.5000e-04\n",
      "Epoch 28/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9797 - loss: 0.0934 - precision: 0.9846 - recall: 0.9746\n",
      "Epoch 28: val_loss improved from 0.07443 to 0.07234, saving model to C:\\Users\\Dinupa Devinda\\Desktop\\ML\\Horn\\horn_cnn_best.keras\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - accuracy: 0.9766 - loss: 0.0997 - precision: 0.9857 - recall: 0.9673 - val_accuracy: 0.9912 - val_loss: 0.0723 - val_precision: 1.0000 - val_recall: 0.9825 - learning_rate: 2.5000e-04\n",
      "Epoch 29/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9695 - loss: 0.1216 - precision: 0.9791 - recall: 0.9604\n",
      "Epoch 29: val_loss did not improve from 0.07234\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 113ms/step - accuracy: 0.9704 - loss: 0.1139 - precision: 0.9719 - recall: 0.9688 - val_accuracy: 0.9561 - val_loss: 0.1256 - val_precision: 0.9194 - val_recall: 1.0000 - learning_rate: 2.5000e-04\n",
      "Epoch 30/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9610 - loss: 0.1290 - precision: 0.9724 - recall: 0.9506\n",
      "Epoch 30: val_loss did not improve from 0.07234\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 0.9603 - loss: 0.1370 - precision: 0.9698 - recall: 0.9502 - val_accuracy: 0.9561 - val_loss: 0.1044 - val_precision: 0.9194 - val_recall: 1.0000 - learning_rate: 2.5000e-04\n",
      "Epoch 31/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9656 - loss: 0.1262 - precision: 0.9818 - recall: 0.9493\n",
      "Epoch 31: val_loss improved from 0.07234 to 0.06089, saving model to C:\\Users\\Dinupa Devinda\\Desktop\\ML\\Horn\\horn_cnn_best.keras\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - accuracy: 0.9579 - loss: 0.1284 - precision: 0.9696 - recall: 0.9455 - val_accuracy: 0.9912 - val_loss: 0.0609 - val_precision: 1.0000 - val_recall: 0.9825 - learning_rate: 2.5000e-04\n",
      "Epoch 32/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9749 - loss: 0.1059 - precision: 0.9850 - recall: 0.9653\n",
      "Epoch 32: val_loss did not improve from 0.06089\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 0.9743 - loss: 0.1045 - precision: 0.9795 - recall: 0.9688 - val_accuracy: 0.9825 - val_loss: 0.0878 - val_precision: 1.0000 - val_recall: 0.9649 - learning_rate: 2.5000e-04\n",
      "Epoch 33/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9792 - loss: 0.0991 - precision: 0.9841 - recall: 0.9742\n",
      "Epoch 33: val_loss did not improve from 0.06089\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 113ms/step - accuracy: 0.9798 - loss: 0.0934 - precision: 0.9873 - recall: 0.9720 - val_accuracy: 0.8947 - val_loss: 0.2359 - val_precision: 1.0000 - val_recall: 0.7895 - learning_rate: 2.5000e-04\n",
      "Epoch 34/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9730 - loss: 0.1042 - precision: 0.9674 - recall: 0.9792\n",
      "Epoch 34: val_loss did not improve from 0.06089\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step - accuracy: 0.9720 - loss: 0.1077 - precision: 0.9690 - recall: 0.9751 - val_accuracy: 0.9649 - val_loss: 0.1143 - val_precision: 1.0000 - val_recall: 0.9298 - learning_rate: 2.5000e-04\n",
      "Epoch 35/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9790 - loss: 0.0863 - precision: 0.9861 - recall: 0.9695\n",
      "Epoch 35: val_loss did not improve from 0.06089\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 113ms/step - accuracy: 0.9766 - loss: 0.0980 - precision: 0.9781 - recall: 0.9751 - val_accuracy: 0.8772 - val_loss: 0.3823 - val_precision: 1.0000 - val_recall: 0.7544 - learning_rate: 2.5000e-04\n",
      "Epoch 36/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9682 - loss: 0.1141 - precision: 0.9854 - recall: 0.9504\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.06089\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 113ms/step - accuracy: 0.9727 - loss: 0.1080 - precision: 0.9887 - recall: 0.9564 - val_accuracy: 0.9649 - val_loss: 0.1154 - val_precision: 1.0000 - val_recall: 0.9298 - learning_rate: 2.5000e-04\n",
      "Epoch 37/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9818 - loss: 0.0951 - precision: 0.9793 - recall: 0.9855\n",
      "Epoch 37: val_loss improved from 0.06089 to 0.05979, saving model to C:\\Users\\Dinupa Devinda\\Desktop\\ML\\Horn\\horn_cnn_best.keras\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - accuracy: 0.9805 - loss: 0.0899 - precision: 0.9768 - recall: 0.9844 - val_accuracy: 0.9912 - val_loss: 0.0598 - val_precision: 0.9828 - val_recall: 1.0000 - learning_rate: 1.2500e-04\n",
      "Epoch 38/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9735 - loss: 0.1176 - precision: 0.9815 - recall: 0.9663\n",
      "Epoch 38: val_loss did not improve from 0.05979\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 0.9805 - loss: 0.0912 - precision: 0.9858 - recall: 0.9751 - val_accuracy: 0.9649 - val_loss: 0.0959 - val_precision: 1.0000 - val_recall: 0.9298 - learning_rate: 1.2500e-04\n",
      "Epoch 39/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9779 - loss: 0.0808 - precision: 0.9790 - recall: 0.9772\n",
      "Epoch 39: val_loss improved from 0.05979 to 0.05545, saving model to C:\\Users\\Dinupa Devinda\\Desktop\\ML\\Horn\\horn_cnn_best.keras\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 115ms/step - accuracy: 0.9782 - loss: 0.0814 - precision: 0.9812 - recall: 0.9751 - val_accuracy: 1.0000 - val_loss: 0.0554 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 1.2500e-04\n",
      "Epoch 40/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9940 - loss: 0.0666 - precision: 0.9977 - recall: 0.9903\n",
      "Epoch 40: val_loss did not improve from 0.05545\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 113ms/step - accuracy: 0.9860 - loss: 0.0773 - precision: 0.9890 - recall: 0.9829 - val_accuracy: 0.9912 - val_loss: 0.0594 - val_precision: 0.9828 - val_recall: 1.0000 - learning_rate: 1.2500e-04\n",
      "Epoch 41/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9779 - loss: 0.0954 - precision: 0.9762 - recall: 0.9789\n",
      "Epoch 41: val_loss did not improve from 0.05545\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 113ms/step - accuracy: 0.9743 - loss: 0.1007 - precision: 0.9750 - recall: 0.9735 - val_accuracy: 1.0000 - val_loss: 0.0633 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 1.2500e-04\n",
      "Epoch 42/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9763 - loss: 0.0931 - precision: 0.9837 - recall: 0.9695\n",
      "Epoch 42: val_loss did not improve from 0.05545\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 0.9751 - loss: 0.0932 - precision: 0.9751 - recall: 0.9751 - val_accuracy: 0.9912 - val_loss: 0.0608 - val_precision: 0.9828 - val_recall: 1.0000 - learning_rate: 1.2500e-04\n",
      "Epoch 43/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9836 - loss: 0.0767 - precision: 0.9961 - recall: 0.9707\n",
      "Epoch 43: val_loss did not improve from 0.05545\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 0.9821 - loss: 0.0821 - precision: 0.9905 - recall: 0.9735 - val_accuracy: 0.9474 - val_loss: 0.1341 - val_precision: 1.0000 - val_recall: 0.8947 - learning_rate: 1.2500e-04\n",
      "Epoch 44/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.9736 - loss: 0.0894 - precision: 0.9798 - recall: 0.9641\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.05545\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - accuracy: 0.9743 - loss: 0.0913 - precision: 0.9856 - recall: 0.9626 - val_accuracy: 0.9474 - val_loss: 0.1591 - val_precision: 1.0000 - val_recall: 0.8947 - learning_rate: 1.2500e-04\n",
      "Epoch 45/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.9858 - loss: 0.0759 - precision: 0.9835 - recall: 0.9873\n",
      "Epoch 45: val_loss did not improve from 0.05545\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 115ms/step - accuracy: 0.9852 - loss: 0.0734 - precision: 0.9906 - recall: 0.9798 - val_accuracy: 0.9561 - val_loss: 0.1022 - val_precision: 1.0000 - val_recall: 0.9123 - learning_rate: 6.2500e-05\n",
      "Epoch 46/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.9868 - loss: 0.0711 - precision: 0.9836 - recall: 0.9899\n",
      "Epoch 46: val_loss did not improve from 0.05545\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - accuracy: 0.9844 - loss: 0.0756 - precision: 0.9844 - recall: 0.9844 - val_accuracy: 0.9649 - val_loss: 0.0897 - val_precision: 1.0000 - val_recall: 0.9298 - learning_rate: 6.2500e-05\n",
      "Epoch 47/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.9915 - loss: 0.0661 - precision: 0.9911 - recall: 0.9924\n",
      "Epoch 47: val_loss did not improve from 0.05545\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - accuracy: 0.9829 - loss: 0.0828 - precision: 0.9814 - recall: 0.9844 - val_accuracy: 0.9912 - val_loss: 0.0577 - val_precision: 1.0000 - val_recall: 0.9825 - learning_rate: 6.2500e-05\n",
      "Epoch 48/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9872 - loss: 0.0732 - precision: 0.9942 - recall: 0.9807\n",
      "Epoch 48: val_loss did not improve from 0.05545\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 113ms/step - accuracy: 0.9852 - loss: 0.0768 - precision: 0.9890 - recall: 0.9813 - val_accuracy: 0.9825 - val_loss: 0.0798 - val_precision: 1.0000 - val_recall: 0.9649 - learning_rate: 6.2500e-05\n",
      "Epoch 49/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9861 - loss: 0.0768 - precision: 0.9967 - recall: 0.9746\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.05545\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - accuracy: 0.9852 - loss: 0.0756 - precision: 0.9952 - recall: 0.9751 - val_accuracy: 0.9649 - val_loss: 0.1281 - val_precision: 1.0000 - val_recall: 0.9298 - learning_rate: 6.2500e-05\n",
      "Epoch 50/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9822 - loss: 0.0863 - precision: 0.9931 - recall: 0.9728\n",
      "Epoch 50: val_loss improved from 0.05545 to 0.04988, saving model to C:\\Users\\Dinupa Devinda\\Desktop\\ML\\Horn\\horn_cnn_best.keras\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - accuracy: 0.9836 - loss: 0.0837 - precision: 0.9859 - recall: 0.9813 - val_accuracy: 1.0000 - val_loss: 0.0499 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 3.1250e-05\n",
      "Epoch 51/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9846 - loss: 0.0805 - precision: 0.9905 - recall: 0.9809\n",
      "Epoch 51: val_loss did not improve from 0.04988\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 0.9860 - loss: 0.0790 - precision: 0.9860 - recall: 0.9860 - val_accuracy: 1.0000 - val_loss: 0.0521 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 3.1250e-05\n",
      "Epoch 52/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9870 - loss: 0.0689 - precision: 0.9908 - recall: 0.9836\n",
      "Epoch 52: val_loss did not improve from 0.04988\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 113ms/step - accuracy: 0.9821 - loss: 0.0804 - precision: 0.9859 - recall: 0.9782 - val_accuracy: 0.9912 - val_loss: 0.0615 - val_precision: 1.0000 - val_recall: 0.9825 - learning_rate: 3.1250e-05\n",
      "Epoch 53/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9934 - loss: 0.0613 - precision: 0.9973 - recall: 0.9900\n",
      "Epoch 53: val_loss did not improve from 0.04988\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 113ms/step - accuracy: 0.9914 - loss: 0.0616 - precision: 0.9953 - recall: 0.9875 - val_accuracy: 0.9912 - val_loss: 0.0611 - val_precision: 1.0000 - val_recall: 0.9825 - learning_rate: 3.1250e-05\n",
      "Epoch 54/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9802 - loss: 0.0790 - precision: 0.9827 - recall: 0.9753\n",
      "Epoch 54: val_loss did not improve from 0.04988\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - accuracy: 0.9836 - loss: 0.0702 - precision: 0.9844 - recall: 0.9829 - val_accuracy: 1.0000 - val_loss: 0.0520 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 3.1250e-05\n",
      "Epoch 55/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9809 - loss: 0.0723 - precision: 0.9819 - recall: 0.9793\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.04988\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 0.9844 - loss: 0.0698 - precision: 0.9905 - recall: 0.9782 - val_accuracy: 0.9912 - val_loss: 0.0644 - val_precision: 1.0000 - val_recall: 0.9825 - learning_rate: 3.1250e-05\n",
      "Epoch 56/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9847 - loss: 0.0648 - precision: 0.9817 - recall: 0.9877\n",
      "Epoch 56: val_loss did not improve from 0.04988\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 0.9860 - loss: 0.0698 - precision: 0.9890 - recall: 0.9829 - val_accuracy: 0.9912 - val_loss: 0.0632 - val_precision: 1.0000 - val_recall: 0.9825 - learning_rate: 1.5625e-05\n",
      "Epoch 57/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9846 - loss: 0.0697 - precision: 0.9879 - recall: 0.9813\n",
      "Epoch 57: val_loss did not improve from 0.04988\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - accuracy: 0.9844 - loss: 0.0690 - precision: 0.9859 - recall: 0.9829 - val_accuracy: 0.9737 - val_loss: 0.0838 - val_precision: 1.0000 - val_recall: 0.9474 - learning_rate: 1.5625e-05\n",
      "Epoch 58/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9802 - loss: 0.0741 - precision: 0.9903 - recall: 0.9711\n",
      "Epoch 58: val_loss did not improve from 0.04988\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 0.9860 - loss: 0.0690 - precision: 0.9890 - recall: 0.9829 - val_accuracy: 0.9825 - val_loss: 0.0700 - val_precision: 1.0000 - val_recall: 0.9649 - learning_rate: 1.5625e-05\n",
      "Epoch 59/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.9903 - loss: 0.0617 - precision: 0.9862 - recall: 0.9941\n",
      "Epoch 59: val_loss did not improve from 0.04988\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 115ms/step - accuracy: 0.9914 - loss: 0.0599 - precision: 0.9922 - recall: 0.9907 - val_accuracy: 0.9912 - val_loss: 0.0597 - val_precision: 1.0000 - val_recall: 0.9825 - learning_rate: 1.5625e-05\n",
      "Epoch 60/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9930 - loss: 0.0623 - precision: 0.9939 - recall: 0.9924\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.04988\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - accuracy: 0.9907 - loss: 0.0651 - precision: 0.9907 - recall: 0.9907 - val_accuracy: 0.9912 - val_loss: 0.0569 - val_precision: 1.0000 - val_recall: 0.9825 - learning_rate: 1.5625e-05\n",
      "Epoch 61/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9881 - loss: 0.0697 - precision: 0.9973 - recall: 0.9794\n",
      "Epoch 61: val_loss did not improve from 0.04988\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 113ms/step - accuracy: 0.9899 - loss: 0.0677 - precision: 0.9953 - recall: 0.9844 - val_accuracy: 1.0000 - val_loss: 0.0539 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 7.8125e-06\n",
      "Epoch 62/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9846 - loss: 0.0709 - precision: 0.9957 - recall: 0.9739\n",
      "Epoch 62: val_loss did not improve from 0.04988\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - accuracy: 0.9844 - loss: 0.0706 - precision: 0.9921 - recall: 0.9766 - val_accuracy: 1.0000 - val_loss: 0.0507 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 7.8125e-06\n",
      "Epoch 63/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9843 - loss: 0.0728 - precision: 0.9839 - recall: 0.9850\n",
      "Epoch 63: val_loss did not improve from 0.04988\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 113ms/step - accuracy: 0.9844 - loss: 0.0752 - precision: 0.9875 - recall: 0.9813 - val_accuracy: 1.0000 - val_loss: 0.0552 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 7.8125e-06\n",
      "Epoch 64/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9711 - loss: 0.0940 - precision: 0.9609 - recall: 0.9803\n",
      "Epoch 64: val_loss did not improve from 0.04988\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 113ms/step - accuracy: 0.9805 - loss: 0.0807 - precision: 0.9798 - recall: 0.9813 - val_accuracy: 0.9912 - val_loss: 0.0574 - val_precision: 1.0000 - val_recall: 0.9825 - learning_rate: 7.8125e-06\n",
      "Epoch 65/120\n",
      "\u001b[1m40/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9886 - loss: 0.0686 - precision: 0.9900 - recall: 0.9883\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.04988\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 0.9868 - loss: 0.0704 - precision: 0.9875 - recall: 0.9860 - val_accuracy: 0.9912 - val_loss: 0.0604 - val_precision: 1.0000 - val_recall: 0.9825 - learning_rate: 7.8125e-06\n",
      "Epoch 65: early stopping\n",
      "Restoring model weights from the end of the best epoch: 50.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "input_shape = X_train.shape[1:]   \n",
    "weight_decay = 1e-4\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "\n",
    "    # Block 1\n",
    "    layers.Conv2D(32, (3, 3), padding='same',\n",
    "                  kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.2),\n",
    "\n",
    "    # Block 2\n",
    "    layers.Conv2D(64, (3, 3), padding='same',\n",
    "                  kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.3),\n",
    "\n",
    "    # Block 3\n",
    "    layers.Conv2D(128, (3, 3), padding='same',\n",
    "                  kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.3),\n",
    "\n",
    "    # Block 4 (small)\n",
    "    layers.Conv2D(128, (3, 3), padding='same',\n",
    "                  kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(1, activation='sigmoid')   \n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "initial_lr = 1e-3\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=initial_lr)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "BEST_MODEL_PATH = r\"C:\\Users\\Dinupa Devinda\\Desktop\\ML\\Horn\\horn_cnn_best.keras\"\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath=BEST_MODEL_PATH,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train_aug,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=120,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b59888d-c6ec-41ae-9760-6a1bd76de1a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.1158\n",
      "compile_metrics: 0.9753\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step\n",
      "\n",
      "Classification report (threshold=0.5):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9695    0.9815    0.9755       162\n",
      "         1.0     0.9812    0.9691    0.9752       162\n",
      "\n",
      "    accuracy                         0.9753       324\n",
      "   macro avg     0.9754    0.9753    0.9753       324\n",
      "weighted avg     0.9754    0.9753    0.9753       324\n",
      "\n",
      "Confusion matrix:\n",
      "[[159   3]\n",
      " [  5 157]]\n",
      "\n",
      "First 10 test samples:\n",
      "Prob horn = 0.99 | Pred = 1 | True = 1.0\n",
      "Prob horn = 0.84 | Pred = 1 | True = 1.0\n",
      "Prob horn = 1.00 | Pred = 1 | True = 1.0\n",
      "Prob horn = 1.00 | Pred = 1 | True = 1.0\n",
      "Prob horn = 1.00 | Pred = 1 | True = 1.0\n",
      "Prob horn = 0.99 | Pred = 1 | True = 1.0\n",
      "Prob horn = 1.00 | Pred = 1 | True = 1.0\n",
      "Prob horn = 1.00 | Pred = 1 | True = 1.0\n",
      "Prob horn = 1.00 | Pred = 1 | True = 1.0\n",
      "Prob horn = 1.00 | Pred = 1 | True = 1.0\n",
      "Final model saved to: C:\\Users\\Dinupa Devinda\\Desktop\\ML\\Horn\\horn_cnn_model_v2.keras\n",
      "Best val-loss model saved to: C:\\Users\\Dinupa Devinda\\Desktop\\ML\\Horn\\horn_cnn_best.keras\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "test_metrics = model.evaluate(X_test, y_test, verbose=0)\n",
    "for name, value in zip(model.metrics_names, test_metrics):\n",
    "    print(f\"{name}: {value:.4f}\")\n",
    "\n",
    "# Detailed metrics\n",
    "y_probs = model.predict(X_test).flatten()\n",
    "y_pred  = (y_probs >= 0.5).astype(\"int32\")\n",
    "\n",
    "print(\"\\nClassification report (threshold=0.5):\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Quick look at first 10 predictions\n",
    "print(\"\\nFirst 10 test samples:\")\n",
    "for p, prob, label in zip(y_pred[:10], y_probs[:10], y_test[:10]):\n",
    "    print(f\"Prob horn = {prob:.2f} | Pred = {p} | True = {label}\")\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "MODEL_DIR = r\"C:\\Users\\Dinupa Devinda\\Desktop\\ML\\Horn\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, \"horn_cnn_model_v2.keras\")\n",
    "model.save(MODEL_PATH)\n",
    "\n",
    "print(\"Final model saved to:\", MODEL_PATH)\n",
    "print(\"Best val-loss model saved to:\", BEST_MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d67ffd14-1fb4-476a-95c3-e2ad41402470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after training\n",
    "np.savez(r\"C:\\Users\\Dinupa Devinda\\Desktop\\ML\\Horn\\norm_stats.npz\", mean=mean, std=std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dc8a14-756b-49c7-96a0-90ac96af8623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded model: C:\\Users\\Dinupa Devinda\\Desktop\\ML\\Horn\\horn_cnn_best.keras\n",
      "âœ… Loaded norm stats: C:\\Users\\Dinupa Devinda\\Desktop\\ML\\Horn\\norm_stats.npz\n",
      "mean = -43.899112701416016 std = 19.589555741356445\n",
      "\n",
      "ğŸ¤ Continuous horn detection started. Press Ctrl+C to stop.\n",
      "Window = 1.0s | Step = 0.25s | Threshold = 0.7\n",
      "ğŸ”‡ Silence (rms 0.0000)\n",
      "ğŸ”‡ Silence (rms 0.0000)\n",
      "ğŸ”‡ Silence (rms 0.0000)\n",
      "ğŸ”‡ Silence (rms 0.0000)\n",
      "\n",
      "ğŸ›‘ Stopped.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import librosa\n",
    "import sounddevice as sd\n",
    "import tensorflow as tf\n",
    "\n",
    "MODEL_PATH = r\"C:\\Users\\Dinupa Devinda\\Desktop\\ML\\Horn\\Models\\horn_cnn_best.keras\"\n",
    "NORM_PATH  = r\"C:\\Users\\Dinupa Devinda\\Desktop\\ML\\Horn\\Models\\norm_stats.npz\"\n",
    "\n",
    "\n",
    "TARGET_SR  = 44100\n",
    "WINDOW_SEC = 1.0\n",
    "N_SAMPLES  = int(TARGET_SR * WINDOW_SEC)\n",
    "\n",
    "N_MELS     = 128\n",
    "N_FFT      = 1024\n",
    "HOP_LENGTH = 512\n",
    "\n",
    "STEP_SEC   = 0.25   \n",
    "BLOCK_SEC  = 0.05   \n",
    "\n",
    "HORN_THRESHOLD = 0.70\n",
    "\n",
    "horn_model = tf.keras.models.load_model(MODEL_PATH)\n",
    "stats = np.load(NORM_PATH)\n",
    "mean = float(stats[\"mean\"])\n",
    "std  = float(stats[\"std\"])\n",
    "\n",
    "print(\"âœ… Loaded model:\", MODEL_PATH)\n",
    "print(\"âœ… Loaded norm stats:\", NORM_PATH)\n",
    "print(\"mean =\", mean, \"std =\", std)\n",
    "\n",
    "\n",
    "ring = np.zeros(N_SAMPLES, dtype=np.float32)\n",
    "write_pos = 0\n",
    "filled = 0\n",
    "\n",
    "def add_samples(x: np.ndarray):\n",
    "    \"\"\"Add new samples into circular buffer.\"\"\"\n",
    "    global ring, write_pos, filled\n",
    "\n",
    "    x = x.astype(np.float32).flatten()\n",
    "    n = len(x)\n",
    "\n",
    "  \n",
    "    if n >= N_SAMPLES:\n",
    "        ring[:] = x[-N_SAMPLES:]\n",
    "        write_pos = 0\n",
    "        filled = N_SAMPLES\n",
    "        return\n",
    "\n",
    "    end = write_pos + n\n",
    "    if end <= N_SAMPLES:\n",
    "        ring[write_pos:end] = x\n",
    "    else:\n",
    "        part1 = N_SAMPLES - write_pos\n",
    "        ring[write_pos:] = x[:part1]\n",
    "        ring[:end - N_SAMPLES] = x[part1:]\n",
    "\n",
    "    write_pos = (write_pos + n) % N_SAMPLES\n",
    "    filled = min(N_SAMPLES, filled + n)\n",
    "\n",
    "def get_last_window():\n",
    "    \"\"\"Return latest 1s window in correct time order.\"\"\"\n",
    "    if filled < N_SAMPLES:\n",
    "        return None\n",
    "    if write_pos == 0:\n",
    "        return ring.copy()\n",
    "    return np.concatenate([ring[write_pos:], ring[:write_pos]]).copy()\n",
    "\n",
    "\n",
    "def audio_to_model_input(y: np.ndarray):\n",
    "   \n",
    "    mel = librosa.feature.melspectrogram(\n",
    "        y=y,\n",
    "        sr=TARGET_SR,\n",
    "        n_fft=N_FFT,\n",
    "        hop_length=HOP_LENGTH,\n",
    "        n_mels=N_MELS,\n",
    "        power=2.0\n",
    "    )\n",
    "    log_mel = librosa.power_to_db(mel, ref=np.max).astype(np.float32)\n",
    "\n",
    "    \n",
    "    log_mel = (log_mel - mean) / (std + 1e-9)\n",
    "\n",
    "   \n",
    "    return log_mel[np.newaxis, ..., np.newaxis].astype(np.float32)\n",
    "\n",
    "def predict_prob(y_1s: np.ndarray) -> float:\n",
    "    X = audio_to_model_input(y_1s)\n",
    "   \n",
    "    prob = float(horn_model(X, training=False).numpy()[0, 0])\n",
    "    return prob\n",
    "\n",
    "\n",
    "def audio_callback(indata, frames, time_info, status):\n",
    "    if status:\n",
    "    \n",
    "        pass\n",
    "   \n",
    "    add_samples(indata[:, 0])\n",
    "\n",
    "\n",
    "def listen_continuous():\n",
    "    print(\"\\nğŸ¤ Continuous horn detection started. Press Ctrl+C to stop.\")\n",
    "    print(f\"Window = {WINDOW_SEC}s | Step = {STEP_SEC}s | Threshold = {HORN_THRESHOLD}\")\n",
    "\n",
    "    next_infer = time.time() + WINDOW_SEC  \n",
    "    ema = None\n",
    "    alpha = 0.4  \n",
    "\n",
    "    blocksize = int(TARGET_SR * BLOCK_SEC)\n",
    "\n",
    "    with sd.InputStream(\n",
    "        samplerate=TARGET_SR,\n",
    "        channels=1,\n",
    "        dtype=\"float32\",\n",
    "        blocksize=blocksize,\n",
    "        callback=audio_callback\n",
    "    ):\n",
    "        while True:\n",
    "            now = time.time()\n",
    "            if now >= next_infer:\n",
    "                next_infer = now + STEP_SEC\n",
    "\n",
    "                y = get_last_window()\n",
    "                if y is None:\n",
    "                    \n",
    "                    continue\n",
    "\n",
    "               \n",
    "                rms = float(np.sqrt(np.mean(y * y)))\n",
    "                if rms < 0.005:\n",
    "                    print(\"ğŸ”‡ Silence (rms {:.4f})\".format(rms))\n",
    "                    continue\n",
    "\n",
    "                prob = predict_prob(y)\n",
    "\n",
    "               \n",
    "                ema = prob if ema is None else (alpha * prob + (1 - alpha) * ema)\n",
    "\n",
    "                msg = f\"ğŸ”Š Horn prob: {prob:.2f} | smooth: {ema:.2f} | rms: {rms:.4f}\"\n",
    "                if ema >= HORN_THRESHOLD:\n",
    "                    print(msg, \"=> ğŸš—ğŸ”Š HORN DETECTED!\")\n",
    "                else:\n",
    "                    print(msg)\n",
    "\n",
    "            time.sleep(0.01)\n",
    "\n",
    "\n",
    "try:\n",
    "    listen_continuous()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nğŸ›‘ Stopped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c5bdf6-47af-4f20-9194-39ae4915fabe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
